{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPSt/eMB0jgBkSTk2U4mCII"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Hate Speech Detection Model"],"metadata":{"id":"hCok48XtBpwC"}},{"cell_type":"markdown","source":["The term hate speech is understood as any type of verbal, written or behavioural communication that attacks or uses derogatory or discriminatory language against a person or group based on what they are, in other words, based on their religion, ethnicity, nationality, race, colour, ancestry, sex or another identity factor. In this article, I will take you through a hate speech detection model with Machine Learning and Python.\n","\n","Hate Speech Detection is generally a task of sentiment classification. So for training, a model that can classify hate speech from a certain piece of text can be achieved by training it on a data that is generally used to classify sentiments. So for the task of hate speech detection model, I will use the Twitter data."],"metadata":{"id":"bOMALigNBsfI"}},{"cell_type":"markdown","source":["The data set I will use for the hate speech detection model consists of a test and train set. The training package includes a list of 31,962 tweets, a corresponding ID and a tag 0 or 1 for each tweet. The particular sentiment we need to detect in this dataset is whether or not the tweet is based on hate speech."],"metadata":{"id":"QYx4h-8rByS2"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AGVG4lW4Beaa","executionInfo":{"status":"ok","timestamp":1671883845917,"user_tz":-330,"elapsed":1087,"user":{"displayName":"Ramesh D Koppad","userId":"09955402310140774456"}},"outputId":"16cdec66-4236-4176-f978-23a9678a9f8a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training Set: (15790, 3) 15790\n","Test Set: (17197, 2) 17197\n"]}],"source":["import pandas as pd\n","train = pd.read_csv('/content/train.csv')\n","print(\"Training Set:\"% train.columns, train.shape, len(train))\n","test = pd.read_csv('/content/test.csv')\n","print(\"Test Set:\"% test.columns, test.shape, len(test))"]},{"cell_type":"markdown","source":["### Data Cleaning\n","\n","**Data cleaning** is the process of preparing incorrectly formated data for analysis by deleting or modifying the incorrectly formatted data which is generally not necessary or useful for data analysis, as it can hinder the process or provide inaccurate results. Now I will perform the process of data cleaning by using the re library in Python:"],"metadata":{"id":"1KtHjSnACxFB"}},{"cell_type":"code","source":["import re\n","def  clean_text(df, text_field):\n","    df[text_field] = df[text_field].str.lower()\n","    df[text_field] = df[text_field].apply(lambda elem: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", elem))  \n","    return df\n","test_clean = clean_text(test, \"tweet\")\n","train_clean = clean_text(train, \"tweet\")"],"metadata":{"id":"cFJ_1DbACui1","executionInfo":{"status":"ok","timestamp":1671883876797,"user_tz":-330,"elapsed":799,"user":{"displayName":"Ramesh D Koppad","userId":"09955402310140774456"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["### Handling Imbalanced data for Hate Speech Detection Model\n","\n","If you will deeply analyse the task we are working on with context to the data we are using, you will find that the tweets regarding hate speeches are comparatively lesser than others, so this is a situation of an unbalanced data.\n","\n","If we will fit this data to train our hate speech detection model, then the model will not generalize any hate speech because the data with context to the hate speech is very less than the positive ones. So in this situation, we need to prepare the data to fit properly in our model."],"metadata":{"id":"EzljPaKBC40j"}},{"cell_type":"markdown","source":["There are a number of methods you can use to deal with this. One approach is to use either oversampling or downsampling. In the case of oversampling, we use a function that repeatedly samples, with replacement, from the minority class until the class is the same size as the majority. Let’s see how we can handle this:"],"metadata":{"id":"zzZfqb1BC8hq"}},{"cell_type":"code","source":["from sklearn.utils import resample\n","train_majority = train_clean[train_clean.label==0]\n","train_minority = train_clean[train_clean.label==1]\n","train_minority_upsampled = resample(train_minority, \n","                                 replace=True,    \n","                                 n_samples=len(train_majority),   \n","                                 random_state=123)\n","train_upsampled = pd.concat([train_minority_upsampled, train_majority])\n","train_upsampled['label'].value_counts()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5mnaF2DkC2Jt","executionInfo":{"status":"ok","timestamp":1671883914229,"user_tz":-330,"elapsed":1318,"user":{"displayName":"Ramesh D Koppad","userId":"09955402310140774456"}},"outputId":"7fcfdd5c-4cfd-425f-f78a-603ab12767a6"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1    14688\n","0    14688\n","Name: label, dtype: int64"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["### Creating a Pipeline\n","\n","For simplicity and reproducibility of the hate speech detection model, I will use the Scikit-Learn’s pipeline with an SGDClassifier, before training our model:"],"metadata":{"id":"Y-hll1IPDBFf"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.pipeline import Pipeline\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.linear_model import SGDClassifier\n","pipeline_sgd = Pipeline([\n","    ('vect', CountVectorizer()),\n","    ('tfidf',  TfidfTransformer()),\n","    ('nb', SGDClassifier()),])"],"metadata":{"id":"SLBOM4-OC-8H","executionInfo":{"status":"ok","timestamp":1671883939104,"user_tz":-330,"elapsed":421,"user":{"displayName":"Ramesh D Koppad","userId":"09955402310140774456"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["### Training the Hate Speech Detection Model\n","\n","Now, before training the model, let’s split the data into a training set and a test set:"],"metadata":{"id":"gmUPSIqvDHMa"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(train_upsampled['tweet'],\n","                                                    train_upsampled['label'],random_state = 0)"],"metadata":{"id":"ckXHNl7TDFY-","executionInfo":{"status":"ok","timestamp":1671883960744,"user_tz":-330,"elapsed":19,"user":{"displayName":"Ramesh D Koppad","userId":"09955402310140774456"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["Now let’s train the model and predict the results on the test set using the F1 score method:"],"metadata":{"id":"PTtFpJ5ZDMOb"}},{"cell_type":"code","source":["model = pipeline_sgd.fit(X_train, y_train)\n","y_predict = model.predict(X_test)\n","from sklearn.metrics import f1_score\n","f1_score(y_test, y_predict)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mfWb1S4YDKwC","executionInfo":{"status":"ok","timestamp":1671883977376,"user_tz":-330,"elapsed":1120,"user":{"displayName":"Ramesh D Koppad","userId":"09955402310140774456"}},"outputId":"9628074a-2a92-4bf4-c9d2-e7b3bbe761be"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.980562729373386"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["So we got an F1 score of 0.98 per cent which is generally appreciatable. This model can now be deployed and used in production."],"metadata":{"id":"sNj_SncoDTNQ"}}]}